{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison with DeepSeek API",
   "id": "c151de2dfa10ba7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Out of curiosity, let us compare our results to results obtained through an API",
   "id": "ec789b29809f29a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install python-dotenv pandas numpy scikit-learn",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:14.393675Z",
     "start_time": "2025-08-25T19:24:07.130596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ],
   "id": "3b375477b318779c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:26.665428Z",
     "start_time": "2025-08-25T19:24:14.401390Z"
    }
   },
   "cell_type": "code",
   "source": "from functions import load_jsonl, combine_data, extract_dict_from_response, compute_f1",
   "id": "92d6ab7f81bb5407",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global Variables",
   "id": "bb1069601a1a5981"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:52:51.883905Z",
     "start_time": "2025-08-25T19:52:51.874754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#data\n",
    "train_file_path = Path('data_germeval/train.jsonl')\n",
    "dev_file_path = Path('data_germeval/development.jsonl')\n",
    "\n",
    "#API key\n",
    "load_dotenv()\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY_25\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY_25\")\n",
    "\n",
    "#API model\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "openAI_model = \"gpt-4o\"\n",
    "\n",
    "#API url\n",
    "deepseek_api_url = \"https://api.deepseek.com/v1/chat/completions\"  \n",
    "openAI_api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "#API data tuple\n",
    "deepseek = (DEEPSEEK_API_KEY, deepseek_model, deepseek_api_url)\n",
    "openAI = (OPENAI_API_KEY, openAI_model, openAI_api_url)\n",
    "\n",
    "#Do we want to use the API?\n",
    "run_this = False"
   ],
   "id": "18a29644566db932",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:27.664983Z",
     "start_time": "2025-08-25T19:24:27.656131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#did the API key load?\n",
    "print(f\"API Key loaded: {DEEPSEEK_API_KEY is not None}\")\n",
    "print(f\"API Key loaded: {OPENAI_API_KEY is not None}\")"
   ],
   "id": "9f036f89a6eaa523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: True\n",
      "API Key loaded: True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First we have to load our data",
   "id": "e8ec0b0c1f38fe11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:27.851817Z",
     "start_time": "2025-08-25T19:24:27.795521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = load_jsonl(train_file_path)\n",
    "dev_data = load_jsonl(dev_file_path)"
   ],
   "id": "ed32551fa2b6b175",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Second we have to generate the bins as in germeval.ipybn",
   "id": "5a1f1a88fabef155"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:31.529342Z",
     "start_time": "2025-08-25T19:24:31.474876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data_labeled = combine_data(train_data)\n",
    "dev_data_labeled = combine_data(dev_data)\n",
    "dev_df = combine_data(dev_data, dataframe = True)"
   ],
   "id": "82a9690a1f5f02a7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now the API part",
   "id": "63b5c1363b1661ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's do it the 'simple' way: We directly provide the AI with a few examples and check what it does.",
   "id": "466f9cb1e1b64c3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DeepSeek first!",
   "id": "60c926bf9f494369"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's do a test",
   "id": "8bbd6709dcf68649"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T11:07:30.983315Z",
     "start_time": "2025-08-24T11:07:30.969144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len_train = len(train_data_labeled)\n",
    "random_indices = np.random.randint(0, len_train, size=100) \n",
    "example_texts = [train_data_labeled[i] for i in random_indices]\n",
    "prompt_english = f\"\"\"\n",
    "**Task:** Predict sexism annotation labels for a new text based on the following label definitions.\n",
    "\n",
    "**Label Definitions:**\n",
    "- 'bin_maj_label': A majority of annotators found the text to be sexist.\n",
    "- 'bin_one_label': At least one annotator found the text to be sexist.\n",
    "- 'bin_all_label': All annotators found the text to be sexist.\n",
    "- 'multi_maj_label': The multi-class label (integer from 0 to 4) that the most annotators assigned.\n",
    "- 'disagree_bin_label': The annotators disagreed on the binary (sexist/not sexist) classification.\n",
    "\n",
    "**Examples from the Dataset:**\n",
    "{chr(10).join(str(example) for example in example_texts)}\n",
    "\n",
    "**Text to Analyze:**\n",
    "'{dev_data_labeled[4]['text']}'\n",
    "\n",
    "**Instructions:**\n",
    "Analyze the text above and predict its labels. Return ONLY a valid Python dictionary in the following format:\n",
    "{{'bin_maj_label': <value>, 'bin_one_label': <value>, 'bin_all_label': <value>, 'multi_maj_label': <value>, 'disagree_bin_label': <value>}}\n",
    "\"\"\""
   ],
   "id": "37e836d69b810ebb",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T11:07:39.733092Z",
     "start_time": "2025-08-24T11:07:31.746280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": deepseek_model, \n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt_english},\n",
    "    ],\n",
    "    \"max_tokens\": 70\n",
    "}\n",
    "response = requests.post(deepseek_api_url, headers=headers, json=data)"
   ],
   "id": "12aa0e91b21d4a38",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T11:07:39.763666Z",
     "start_time": "2025-08-24T11:07:39.754882Z"
    }
   },
   "cell_type": "code",
   "source": "response.json()['choices'][0]['message']['content']",
   "id": "25ce8ec5de3da3b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'bin_maj_label': 1, 'bin_one_label': 1, 'bin_all_label': 0, 'multi_maj_label': 2, 'disagree_bin_label': 1}\""
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's generalize",
   "id": "486a41a80f857b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Zero Shot:",
   "id": "ebd20f7ceeb84fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:28:13.212202Z",
     "start_time": "2025-08-25T19:28:13.206283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_prompt_zero_shot(text_to_analyze):\n",
    "    \"\"\"\n",
    "    generates a fitting prompt\n",
    "    :param text_to_analyze: str, text that is to be analyzed\n",
    "    :return: str, prompt to send to the API\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Predict sexism annotation labels for a new text based on the following label definitions.\n",
    "    \n",
    "    **Label Definitions:**\n",
    "    - 'bin_maj_label': A majority of annotators found the text to be sexist.\n",
    "    - 'bin_one_label': At least one annotator found the text to be sexist.\n",
    "    - 'bin_all_label': All annotators found the text to be sexist.\n",
    "    - 'multi_maj_label': The multi-class label (integer from 0 to 4) that the most annotators assigned.\n",
    "    - 'disagree_bin_label': The annotators disagreed on the binary (sexist/not sexist) classification.\n",
    "    \n",
    "    **Text to Analyze:**\n",
    "    '{text_to_analyze}'\n",
    "    \n",
    "    **Instructions:**\n",
    "    Analyze the text above and predict its labels. Return ONLY a valid Python dictionary in exactly the following format \n",
    "    (no spaces or newlines!). <value> must always be an integer::\n",
    "    {{'bin_maj_label': <value>, 'bin_one_label': <value>, 'bin_all_label': <value>, 'multi_maj_label': <value>, 'disagree_bin_label': <value>}}\n",
    "    \"\"\"\n",
    "    return prompt"
   ],
   "id": "4649591b836b0ec2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:46.823947Z",
     "start_time": "2025-08-25T19:24:46.817783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_labels_zero_shot(text_to_analyze, api_key, model, api_url):\n",
    "    \"\"\"\n",
    "    Generates a prediction for a given text via the DeepSeek API\n",
    "    :param text_to_analyze: str, text that is to be analyzed\n",
    "    :param api_key: API key\n",
    "    :param model: str, name of model\n",
    "    :param api_url: str, link for model\n",
    "    :return: str, answer of the LLM\n",
    "    \"\"\"  \n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    prompt = get_prompt_zero_shot(text_to_analyze)\n",
    "    dat = {\n",
    "    \"model\": model, \n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    \"max_tokens\": 70\n",
    "    }\n",
    "    respo = requests.post(api_url, headers=headers, json=dat)\n",
    "    return respo.json()['choices'][0]['message']['content']"
   ],
   "id": "2f8d0441dc232eb4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:47.946120Z",
     "start_time": "2025-08-25T19:24:47.932132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_string_zero_shot(item,  api_key, model, api_url):\n",
    "    \"\"\"\n",
    "    combines the whole process\n",
    "    :param item: dictionary containing our data\n",
    "    :param api_key: API key\n",
    "    :param model: str, name of model\n",
    "    :param api_url: str, link for model\n",
    "    :return: dictionary with our results\n",
    "    \"\"\"\n",
    "    #print(item)\n",
    "    result = generate_labels_zero_shot(item['text'],  api_key, model, api_url)\n",
    "    result = extract_dict_from_response(result)\n",
    "    while not result[0]:\n",
    "        result = extract_dict_from_response(generate_labels_zero_shot(item['text'],  api_key, model, api_url))\n",
    "    eval_dict = dict()\n",
    "    eval_dict['id'] = item['id']\n",
    "    eval_dict['text'] = item['text']\n",
    "    eval_dict.update(result[1])\n",
    "    return eval_dict"
   ],
   "id": "4abff595385beac",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_this:\n",
    "    dev_data_predicted_0 = [eval_string_zero_shot(item, *deepseek) for item in dev_data[:100]]\n",
    "    dev_df_predicted_0 = pd.DataFrame(dev_data_predicted_0, columns=dev_data_predicted_0[0].keys())"
   ],
   "id": "62b5be62e9c715cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T15:23:13.078120Z",
     "start_time": "2025-08-24T15:23:13.049839Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_0)",
   "id": "20814d0a2eb48c88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.6966680446465482\n",
      "Dev set F1 score Bin One: 0.749474527074367\n",
      "Dev set F1 score Bin All: 0.8238297872340425\n",
      "Dev set F1 score Multi Maj: 0.5660397497239602\n",
      "Dev set F1 score Disagree Bin: 0.6165782044042915\n"
     ]
    }
   ],
   "execution_count": 339
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These results are ok, but maybe we can improve them by giving DeepSeek some examples?",
   "id": "eee21508edb1f076"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Few Shot",
   "id": "57a3359b8916b66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:53.923868Z",
     "start_time": "2025-08-25T19:24:53.917738Z"
    }
   },
   "cell_type": "code",
   "source": "len_train = len(train_data_labeled)",
   "id": "6f952f9b77478a29",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:28:05.134941Z",
     "start_time": "2025-08-25T19:28:05.119990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_prompt(text_to_analyze, num_examples):\n",
    "    \"\"\"\n",
    "    generates a fitting prompt. Includes a number of examples randomly picked from the training data\n",
    "    :param text_to_analyze: str, text that is to be analyzed\n",
    "    :param num_examples: int, number of examples to include in the prompt\n",
    "    :return: str, prompt to send to the API\n",
    "    \"\"\"\n",
    "    random_indices = np.random.randint(0, len_train, size=num_examples) \n",
    "    example_texts = [train_data_labeled[i] for i in random_indices]\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Predict sexism annotation labels for a new text based on the following label definitions.\n",
    "    \n",
    "    **Label Definitions:**\n",
    "    - 'bin_maj_label': A majority of annotators found the text to be sexist.\n",
    "    - 'bin_one_label': At least one annotator found the text to be sexist.\n",
    "    - 'bin_all_label': All annotators found the text to be sexist.\n",
    "    - 'multi_maj_label': The multi-class label (integer from 0 to 4) that the most annotators assigned.\n",
    "    - 'disagree_bin_label': The annotators disagreed on the binary (sexist/not sexist) classification.\n",
    "    \n",
    "    **Examples from the Dataset:**\n",
    "    {chr(10).join(str(example) for example in example_texts)}\n",
    "    \n",
    "    **Text to Analyze:**\n",
    "    '{text_to_analyze}'\n",
    "    \n",
    "    **Instructions:**\n",
    "    Analyze the text above and predict its labels. Return ONLY a valid Python dictionary in exactly the following format \n",
    "    (no spaces or newlines!). <value> must always be an integer::\n",
    "    {{'bin_maj_label': <value>, 'bin_one_label': <value>, 'bin_all_label': <value>, 'multi_maj_label': <value>, 'disagree_bin_label': <value>}}\n",
    "    \"\"\"\n",
    "    return prompt"
   ],
   "id": "4c4a56860f0dcd24",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:55.164941Z",
     "start_time": "2025-08-25T19:24:55.159313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_labels(text_to_analyze, num_examples, api_key, model, api_url):\n",
    "    \"\"\"\n",
    "    Generates a prediction for a given text via the DeepSeek API\n",
    "    :param text_to_analyze: str, text that is to be analyzed\n",
    "    :param num_examples: int, number of examples to include in the prompt\n",
    "    :param api_key: API key\n",
    "    :param model: str, name of model\n",
    "    :param api_url: str, link for model\n",
    "    :return: str, answer of the LLM\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    prompt = get_prompt(text_to_analyze, num_examples)\n",
    "    dat = {\n",
    "    \"model\": model, \n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    \"max_tokens\": 70\n",
    "    }\n",
    "    respo = requests.post(api_url, headers=headers, json=dat)\n",
    "    return respo.json()['choices'][0]['message']['content']"
   ],
   "id": "a603d8bdba7d778b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:24:56.338358Z",
     "start_time": "2025-08-25T19:24:56.329342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_string(item, num_examples, api_key, model, api_url):\n",
    "    \"\"\"\n",
    "    combines the whole process\n",
    "    :param item: dictionary containing our data\n",
    "    :param num_examples: int, number of examples to include in the prompt  \n",
    "    :param api_key: API key\n",
    "    :param model: str, name of model\n",
    "    :param api_url: str, link for model \n",
    "    :return: dictionary with our results\n",
    "    \"\"\"\n",
    "    #print(item)\n",
    "    result = extract_dict_from_response(generate_labels(item['text'], num_examples, api_key, model, api_url))\n",
    "    while not result[0]:\n",
    "        result = extract_dict_from_response(generate_labels(item['text'], num_examples, api_key, model, api_url))\n",
    "    eval_dict = dict()\n",
    "    eval_dict['id'] = item['id']\n",
    "    eval_dict['text'] = item['text']\n",
    "    eval_dict.update(result[1])\n",
    "    return eval_dict"
   ],
   "id": "652a686afeed46c0",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_this:\n",
    "    dev_data_predicted = [eval_string(item, 100, *deepseek) for item in dev_data[:100]]\n",
    "    dev_df_predicted = pd.DataFrame(dev_data_predicted, columns = dev_data_predicted[0].keys())"
   ],
   "id": "20f3752fa4906948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T15:24:07.921966Z",
     "start_time": "2025-08-24T15:24:07.886763Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted)",
   "id": "c0b6ca75f1f85847",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.8191471215351812\n",
      "Dev set F1 score Bin One: 0.8200720288115246\n",
      "Dev set F1 score Bin All: 0.8238297872340425\n",
      "Dev set F1 score Multi Maj: 0.7006038647342995\n",
      "Dev set F1 score Disagree Bin: 0.7059975520195839\n"
     ]
    }
   ],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro: Results not bad\n",
    "Contra: Took quite a while and costs a bit (0.5$). Can we do it with fewer examples?"
   ],
   "id": "d4fb07c2f5f427b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_this:\n",
    "    dev_data_predicted_5 = [eval_string(item, 5, *deepseek) for item in dev_data[:100]]\n",
    "    dev_df_predicted_5 = pd.DataFrame(dev_data_predicted_5, columns = dev_data_predicted_5[0].keys())\n",
    "    dev_data_predicted_10 = [eval_string(item, 10, *deepseek) for item in dev_data[:100]]\n",
    "    dev_df_predicted_10 = pd.DataFrame(dev_data_predicted_10, columns=dev_data_predicted_10[0].keys())"
   ],
   "id": "574399d28c34d16d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T15:24:20.735768Z",
     "start_time": "2025-08-24T15:24:20.708388Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_5)",
   "id": "ba65d378bc84a2be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.78288\n",
      "Dev set F1 score Bin One: 0.82991499149915\n",
      "Dev set F1 score Bin All: 0.813763440860215\n",
      "Dev set F1 score Multi Maj: 0.6724038713910762\n",
      "Dev set F1 score Disagree Bin: 0.735386189258312\n"
     ]
    }
   ],
   "execution_count": 341
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T15:24:22.741185Z",
     "start_time": "2025-08-24T15:24:22.714509Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_10)",
   "id": "bd37282d6c01ceb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.8109375\n",
      "Dev set F1 score Bin One: 0.8197839135654261\n",
      "Dev set F1 score Bin All: 0.8596756756756757\n",
      "Dev set F1 score Multi Maj: 0.676089427891324\n",
      "Dev set F1 score Disagree Bin: 0.7157851662404093\n"
     ]
    }
   ],
   "execution_count": 342
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion (DeepSeek)",
   "id": "e673dcf6ebf23dc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The 5-examples version is slightly worse than the 10-examples version. The 10-examples version is slightly worse than the 100-examples version.\n",
    "10 examples seem to be a good number of examples as performance is almost as good as the 100-example version, but the 10-example version cost ~0.1$"
   ],
   "id": "8fa035a46840dd67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro: Results are better than the prediction of the fine-tuned BERT models.\n",
    "Contra: There are probably issues with reproducibility. One would have to average over many more predictions to truly know if the API yields reliable results."
   ],
   "id": "38bfbfe1490bb35b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's compare the OpenAI API",
   "id": "1056a7dc2adaa329"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOTE: I first tried \"gpt-3.5-turbo\" but results were less good, so this is \"gpt-4o\"",
   "id": "f721eaccf8dcd492"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T19:55:28.672398Z",
     "start_time": "2025-08-25T19:53:00.148038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if run_this:\n",
    "    dev_data_predicted_0_openAI = [eval_string_zero_shot(item, *openAI) for item in dev_data[:100]]\n",
    "    dev_df_predicted_0_openAI = pd.DataFrame(dev_data_predicted_0_openAI, columns=dev_data_predicted_0_openAI[0].keys())\n",
    "    dev_data_predicted_5_openAI = [eval_string(item, 5, *openAI) for item in dev_data[:100]]\n",
    "    dev_df_predicted_5_openAI = pd.DataFrame(dev_data_predicted_5_openAI, columns=dev_data_predicted_5_openAI[0].keys())\n",
    "    dev_data_predicted_10_openAI = [eval_string(item, 10, *openAI) for item in dev_data[:100]] #This can lead to error messages when the model is busy. In this case, either try again, split the data into smaller batches and pause between running them, or switch to an older model (like gpt-3.5-turbo)\n",
    "    dev_df_predicted_10_openAI = pd.DataFrame(dev_data_predicted_10_openAI, columns=dev_data_predicted_10_openAI[0].keys())"
   ],
   "id": "bc75fa2774f6f0f2",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:16:27.036521Z",
     "start_time": "2025-08-25T20:16:26.996547Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_0_openAI)",
   "id": "da5a8fedd580f2ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.7168081494057725\n",
      "Dev set F1 score Bin One: 0.7391666666666665\n",
      "Dev set F1 score Bin All: 0.8238297872340425\n",
      "Dev set F1 score Multi Maj: 0.535944055944056\n",
      "Dev set F1 score Disagree Bin: 0.6632692307692308\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:16:28.022010Z",
     "start_time": "2025-08-25T20:16:27.996140Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_5_openAI)",
   "id": "41afa40e438cf4e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.7356156156156156\n",
      "Dev set F1 score Bin One: 0.7488721804511278\n",
      "Dev set F1 score Bin All: 0.8219864995178399\n",
      "Dev set F1 score Multi Maj: 0.6247619047619047\n",
      "Dev set F1 score Disagree Bin: 0.6177357339029601\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:16:28.832470Z",
     "start_time": "2025-08-25T20:16:28.797863Z"
    }
   },
   "cell_type": "code",
   "source": "compute_f1(dev_df.iloc[:100], dev_df_predicted_10_openAI)",
   "id": "f119c2abea087bd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.7519945725915875\n",
      "Dev set F1 score Bin One: 0.779735894357743\n",
      "Dev set F1 score Bin All: 0.8389743589743591\n",
      "Dev set F1 score Multi Maj: 0.6463893766461808\n",
      "Dev set F1 score Disagree Bin: 0.6666995073891626\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "65d82456be32f798"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OpenAI, like DeepSeek profits from being shown more examples. ",
   "id": "576c3b63727137a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, with the same number of examples, OpenAI yields worse results than DeepSeek. ",
   "id": "8256026cdf31b6e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The 10-examples version of DeepSeek seems to be a good compromise, as it both performs well and is low cost.",
   "id": "d188e80f6374e07e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
